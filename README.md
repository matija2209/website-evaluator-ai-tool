# Website Sophistication Analyzer

## üéâ **PROJECT COMPLETE** - All 4 Phases Delivered + Design Age Estimation!

A comprehensive tool for analyzing company websites to evaluate their digital sophistication and identify improvement opportunities.

**‚úÖ PRODUCTION READY**: Complete end-to-end pipeline from directory listings to AI-powered website analysis with design age estimation

## Overview

This project transforms directory listings (from sources like bizi.si, zlatestrani.si) into actual company websites, captures screenshots, and uses AI analysis to assess website sophistication across multiple criteria including design age estimation.

## Features

- **Website Discovery**: Finds actual company websites from directory listings using Google Custom Search API
- **Screenshot Capture**: Takes sectioned screenshots (desktop & mobile) for comprehensive analysis
- **AI Analysis**: Uses Gemini 2.0 Flash for sophisticated website evaluation
- **üé® Design Age Estimation**: Determines website design era from Early 2000s to Modern 2020s
- **Progress Tracking**: Resume-capable processing with CSV-based progress tracking
- **Batch Processing**: Handles large datasets with rate limiting and error handling

## Project Structure

```
/project-root
  /runs
    /[RUN_ID]/
      /screenshots
        /{domain}/
          /desktop
            section-1.jpeg
            section-2.jpeg
          /mobile
            section-1.jpeg
            section-2.jpeg
      input.csv
      website-discovery-progress.csv
      screenshot-progress.csv
      output.csv
  /src                        # Core CLI business logic
    aiAnalysisService.ts      # Enhanced with design age estimation
    aiValidationService.ts
    csvProcessor.ts
    exclusionLists.ts
    googleSearch.ts
    runManager.ts
    screenshotCapture.ts
    types.ts
    websiteAnalysisRunner.ts
    websiteDiscovery.ts
  /api                        # üÜï Professional API layer (added 2025-01-09)
    server.ts                 # Express API server
    worker.ts                 # BullMQ background worker
    /routes                   # REST API endpoints
      index.ts, health.ts, analysis.ts, jobs.ts, downloads.ts
    /controllers              # Request handling logic
      healthController.ts, analysisController.ts, jobsController.ts, downloadController.ts
    /services                 # Business logic services
      queueService.ts, fileService.ts, analysisService.ts, jobStatusService.ts
    /middleware               # Express middleware
      errorHandler.ts, uploadMiddleware.ts
    /types                    # API-specific TypeScript types
      api.ts, queue.ts
    /config                   # Configuration management
      redis.ts
  /docs
    intro.md
    step-2-custom-search.md
    step-3-screenshoot.md
    step-4-gemini.md
  config.ts
  compose.yml                 # üÜï Docker Compose for production API
  Dockerfile                  # üÜï Containerized API deployment
```

## Installation

```bash
pnpm install
```

### Dependencies

**Core CLI Dependencies:**
- `playwright` - For screenshot capture (browser automation)
- `sharp` - For image compression and processing
- `@google/generative-ai` - Gemini AI integration
- `@googleapis/customsearch` - Google Custom Search API
- `csv-parser` & `csv-writer` - CSV processing
- `axios` - HTTP requests
- `cheerio` - HTML parsing
- `fs-extra` - File system utilities
- `dayjs` - Date handling

**üÜï API Dependencies (added 2025-01-09):**
- `express` - Web framework for REST API
- `bullmq` - Redis-based job queue for background processing
- `ioredis` - Redis client
- `cors` - Cross-origin resource sharing
- `multer` - File upload middleware

## Environment Setup

Create a `.env` file with your API keys:

```env
GOOGLE_CLOUD_API_KEY=your_GOOGLE_CLOUD_API_KEY_here
```

## Usage

### üÜï **API Mode (Production Web Service) - Added 2025-01-09**

Start the production API with Docker (recommended):

```bash
# 1. Create your .env file with API keys
cp .env.example .env
# Edit .env with your GOOGLE_CLOUD_API_KEY

# 2. Start Redis + API + Worker services
docker compose up -d

# 3. Use the REST API
curl -X POST http://localhost:3000/analyze -F "csv=@/path/to/companies.csv"
# Returns: {"jobId": "123", "status": "queued", "estimatedTime": "6-10 hours"}

# 4. Check job status
curl http://localhost:3000/jobs/123

# 5. Download results when complete  
curl http://localhost:3000/download/[RUN_ID] -o results.csv
```

**Available API Endpoints:**
- `GET /health` - Health check
- `POST /analyze` - Upload CSV and start analysis (returns job ID)
- `GET /jobs` - List all jobs
- `GET /jobs/:jobId` - Get specific job status and progress
- `DELETE /jobs/:jobId` - Delete/cancel job
- `GET /download/:runId` - Download results CSV

**Development API Mode:**
```bash
pnpm run dev:api      # API server with hot reload
pnpm run dev:worker   # Background worker with hot reload
```

### **CLI Mode (Direct Processing)**

```bash
# Phase 1 & 2: Website Discovery
pnpm run start /path/to/companies.csv

# Phase 3: Screenshot Capture
pnpm run screenshot [RUN_ID] [concurrency]
pnpm run screenshot 20250629_110643 1

# Phase 4: AI Analysis
pnpm run analyze [RUN_ID] [force]
pnpm run analyze 20250629_110643
pnpm run analyze 20250629_110643 force  # Force reanalysis
```

### Individual Phase Commands

```bash
# Testing commands
pnpm run test                    # Infrastructure tests
pnpm run test:phase2            # Website discovery tests  
pnpm run test:ai-analysis       # AI analysis tests

# Development
pnpm run dev                    # Watch mode for development
pnpm run build                  # TypeScript compilation
pnpm run clean                  # Clean build artifacts

# üÜï API commands (added 2025-01-09)
pnpm run api                    # Start production API server
pnpm run worker                 # Start background worker
pnpm run dev:api               # API development with hot reload
pnpm run dev:worker            # Worker development with hot reload
```

### Input CSV Format

The input CSV should contain company data with these headers:

```
Filter Group,Title,URL,Street Address,City,Business ID,Tax Number,Activity Description,Employee Count,Page
```

**Note**: Input URLs are directory listings, not actual company websites. The tool will discover the actual websites.

## Processing Pipeline

The tool follows a sequential 4-phase approach:

### Phase 1: Foundation
- Environment setup and data preparation
- Creates run-specific directories with timestamp-based RUN_ID

### Phase 2: Website Discovery üî• **CRITICAL PHASE**
- Uses Google Custom Search API to find actual company websites
- AI-enhanced intelligent website selection (vs simple "first result")
- Applies domain exclusion filters (directories, social media)
- Companies without discovered websites are excluded from further processing

### Phase 3: Screenshot Capture
- Captures sectioned screenshots for both desktop and mobile views
- Uses Playwright for reliable browser automation
- Organizes screenshots by domain in structured folders
- Only processes companies with successfully discovered websites

### Phase 4: AI Analysis ‚ú® **ENHANCED WITH DESIGN AGE ESTIMATION**
- Uses Gemini 2.0 Flash for sophisticated website evaluation
- Mobile-first scoring (70% mobile + 30% desktop weighting)
- 4-criteria analysis: Visual Design, Technical, Content, UX
- **üé® Design Age Estimation**: Analyzes design patterns to determine era:
  - **EARLY_2000S**: Table layouts, basic colors
  - **MID_2000S**: Flash elements, limited CSS
  - **LATE_2000S**: CSS frameworks, gradients
  - **EARLY_2010S**: jQuery effects, basic responsive
  - **MID_2010S**: Bootstrap era, flat design
  - **LATE_2010S**: Card layouts, material design
  - **EARLY_2020S**: Modern frameworks, CSS Grid
  - **MODERN_2020S**: Component-based, advanced interactions
- Context-aware recommendations based on business activity
- Only processes companies with successful screenshots

## Output

The main output is in `runs/[RUN_ID]/website-discovery-progress.csv` with all phase results:

**Core Company Data:**
```
Company_Name, Cleaned_Name, City, Original_URL, Discovered_Website
```

**Phase 2 - Website Discovery:**
```
Search_Status, SERP_Position, AI_Confidence, AI_Reasoning, Tokens_Used
```

**Phase 3 - Screenshot Capture:**
```
Screenshot_Status, Desktop_Sections, Mobile_Sections, Load_Time_MS, Screenshot_Timestamp
```

**Phase 4 - AI Analysis (ENHANCED):**
```
Analysis_Status, Mobile_Score, Desktop_Score, Combined_Score, 
Sophistication_Level, Opportunity_Level, Design_Era, Design_Reasoning,
Mobile_Issues, Desktop_Issues, Quick_Wins, Major_Upgrades, 
Analysis_Confidence, Analysis_Reasoning, Analysis_Tokens_Used, Analysis_Timestamp
```

## Configuration

Key settings in `config.ts`:

- **Batch Size**: 15 companies for rate-limited phases
- **Rate Limiting**: 2000ms delays between AI analysis calls
- **Retry Logic**: Up to 3 attempts per phase
- **Google Custom Search**: 10 results per query with 100ms delays
- **üé® AI Analysis**: Enhanced prompts with design era indicators, 1500 token limit

## Error Handling

- **No website found**: Marked as `NO_WEBSITE_FOUND`, excluded from further processing
- **Screenshot failed**: Marked as `SCREENSHOT_FAILED`, excluded from AI analysis
- **Analysis failed**: Marked as `ANALYSIS_FAILED`, included in output with error status

## Resume Capability

Each phase tracks progress in CSV files, allowing the tool to resume from the last successful point if interrupted.

## Development Approach

**Sequential Development**: Each phase must be perfected individually before moving to the next:

1. **Phase 1**: Foundation (Days 1-2)
2. **Phase 2**: Website Discovery (Days 3-5) ‚Üê **CORE PHASE**
3. **Phase 3**: Screenshot Capture (Days 6-8)
4. **Phase 4**: AI Analysis (Days 9-11)
5. **Phase 5**: Integration & Testing (Days 12-15)

## Success Criteria

- **Phase 2**: 60-80% website discovery success rate
- **Phase 3**: 90%+ screenshot success rate for reachable websites
- **Phase 4**: Consistent scoring and token usage within budget
- **Full Pipeline**: Process complete dataset without crashes

## Performance Results

**Live Testing (2 companies):**
- **Phase 2**: 100% website discovery success
- **Phase 3**: 100% screenshot capture success (22 sections)
- **Phase 4**: 100% AI analysis success (~5,500 tokens per website)

**Estimated Production (717 companies):**
- **Total Time**: 6-10 hours for complete pipeline
- **Success Rate**: 70-90% website discovery (~500-650 companies)
- **API Costs**: 
  - Gemini 2.0 Flash: ~$45-90 (increased for design age analysis)
  - Google Custom Search: ~$100-200
  - **Total**: ~$145-290

## Documentation

Detailed implementation guides are available in the `/docs` folder:

- `intro.md` - Complete project overview and specifications
- `step-2-custom-search.md` - Website discovery implementation
- `step-3-screenshoot.md` - Screenshot capture details
- `step-4-gemini.md` - AI analysis implementation

## Development Status

‚úÖ **PROJECT COMPLETE** - All 4 phases implemented and tested + **üÜï Production API Added!**

**Completed Phases:**
- ‚úÖ **Phase 1**: Foundation & Infrastructure
- ‚úÖ **Phase 2**: Website Discovery (AI-Enhanced)
- ‚úÖ **Phase 3**: Screenshot Capture  
- ‚úÖ **Phase 4**: AI Analysis & Scoring + **üé® Design Age Estimation**
- ‚úÖ **üÜï Phase 5**: Production Web API (added 2025-01-09)

**Live Testing Results:**
- 100% success rate across all phases
- 2 Slovenian companies analyzed end-to-end
- Design age estimation working correctly (EARLY_2020S era detected)
- Complete pipeline ready for production deployment
- **üÜï Professional REST API with Docker deployment ready**

---

## üÜï **API Development Session (2025-01-09)**

### **What We Built Today:**

**‚úÖ Professional API Architecture:**
- Clean separation: routes ‚Üí controllers ‚Üí services
- Enterprise-grade folder structure (`/api` directory)
- TypeScript interfaces for API requests/responses
- Proper error handling and validation middleware

**‚úÖ Production Infrastructure:**
- **Docker Compose** setup with Redis 8.2.1 + Node.js 22 LTS
- **BullMQ** job queue for background processing (6-10 hour jobs)
- **Multiple services**: API server + background worker + Redis
- **Environment management** with `.env` file support

**‚úÖ RESTful API Endpoints:**
- `POST /analyze` - Upload CSV, get job ID (non-blocking)
- `GET /jobs/:jobId` - Real-time job status and progress
- `GET /download/:runId` - Download completed results
- `GET /jobs` - List all jobs with summary stats
- `DELETE /jobs/:jobId` - Job cleanup

**‚úÖ Technical Improvements:**
- **Converted CLI to API** without changing core business logic
- **Removed duplicate files** (obsolete `api-server.ts`, `api-worker.ts`)
- **Fixed Docker build** (removed useless `pnpm run build` step)
- **Updated to latest versions** (Node 22 LTS, Redis 8.2.1-alpine)

### **Architecture Highlights:**

```
/api/
‚îú‚îÄ‚îÄ server.ts              # Express API server
‚îú‚îÄ‚îÄ worker.ts             # BullMQ background processor  
‚îú‚îÄ‚îÄ routes/               # Clean REST endpoints
‚îú‚îÄ‚îÄ controllers/          # Request/response handling
‚îú‚îÄ‚îÄ services/            # Business logic (Queue, File, Analysis)
‚îú‚îÄ‚îÄ middleware/          # Error handling, file uploads
‚îú‚îÄ‚îÄ types/              # TypeScript API interfaces
‚îî‚îÄ‚îÄ config/             # Redis/queue configuration
```

### **Key Benefits Achieved:**

**üéØ Dual Mode Operation:**
- **CLI Mode**: Direct processing for development/testing
- **API Mode**: Web service for production deployments

**‚ö° Non-Blocking Processing:**
- Upload CSV ‚Üí Get job ID immediately  
- Check progress in real-time
- Download results when complete

**üèóÔ∏è Production Ready:**
- Docker containerization with health checks
- Redis persistence and job queue management
- Proper error handling and file management
- Professional API documentation

**üí° Zero Business Logic Changes:**
- Your original `/src` pipeline stays 100% unchanged
- Same AI analysis, screenshot capture, website discovery
- API is just a clean wrapper around existing functionality

### **Ready for Production:**
```bash
docker compose up -d  # Start Redis + API + Worker
# Upload 717-company CSV via API
# Monitor progress via REST endpoints  
# Download results when complete
```

The project now supports both **direct CLI usage** for development and **production web API** for scalable deployments! üöÄ

## License

[Your License Here]

## Contributing

[Contributing guidelines if applicable] 